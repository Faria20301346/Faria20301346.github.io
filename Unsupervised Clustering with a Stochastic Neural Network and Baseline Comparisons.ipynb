{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgnoffsuB5Fxkzi61iNHWE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Faria20301346/Faria20301346.github.io/blob/main/Unsupervised%20Clustering%20with%20a%20Stochastic%20Neural%20Network%20and%20Baseline%20Comparisons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "da2ngTUynPcp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Wine dataset from scikit-learn\n",
        "wine_data = load_wine(as_frame=True)\n",
        "wine_df = wine_data.frame\n",
        "print(wine_df.head())\n",
        "print(\"\\nShape of the Wine DataFrame:\", wine_df.shape)\n",
        "\n",
        "# Separate features (X) and target labels (y)\n",
        "X = wine_df.drop('target', axis=1).values\n",
        "y = wine_df['target'].values\n",
        "\n",
        "# Standardize the features to a common scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"\\nShape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKKrDRa7ndLt",
        "outputId": "faa30db4-0322-4c46-8a19-3cbca5d61830"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
            "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
            "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
            "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
            "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
            "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
            "\n",
            "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
            "0        3.06                  0.28             2.29             5.64  1.04   \n",
            "1        2.76                  0.26             1.28             4.38  1.05   \n",
            "2        3.24                  0.30             2.81             5.68  1.03   \n",
            "3        3.49                  0.24             2.18             7.80  0.86   \n",
            "4        2.69                  0.39             1.82             4.32  1.04   \n",
            "\n",
            "   od280/od315_of_diluted_wines  proline  target  \n",
            "0                          3.92   1065.0       0  \n",
            "1                          3.40   1050.0       0  \n",
            "2                          3.17   1185.0       0  \n",
            "3                          3.45   1480.0       0  \n",
            "4                          2.93    735.0       0  \n",
            "\n",
            "Shape of the Wine DataFrame: (178, 14)\n",
            "\n",
            "Shape of X_train: (124, 13)\n",
            "Shape of X_test: (54, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StochasticClusteringNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(StochasticClusteringNN, self).__init__()\n",
        "\n",
        "        # Encoder layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder layers\n",
        "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights with Xavier Normal and biases with zeros for better convergence\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = torch.relu(self.fc1(x))\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = torch.relu(self.fc3(z))\n",
        "        # Use a linear activation as the output is standardized and can be negative\n",
        "        return self.fc4(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_reconstructed = self.decode(z)\n",
        "        return x_reconstructed, z, mu, logvar"
      ],
      "metadata": {
        "id": "X030taF9nifV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(x_reconstructed, x, mu, logvar, beta=1.0):\n",
        "    # Use Mean Squared Error (MSE) for reconstruction loss\n",
        "    reconstruction_loss = nn.MSELoss()(x_reconstructed, x)\n",
        "\n",
        "    # Calculate KL Divergence loss\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # Normalize the KL loss by the batch size\n",
        "    kl_loss /= x.size(0)\n",
        "\n",
        "    # Return the combined loss\n",
        "    return reconstruction_loss + beta * kl_loss"
      ],
      "metadata": {
        "id": "z2YQMi5znmi3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up model parameters\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 64\n",
        "latent_dim = 32\n",
        "model = StochasticClusteringNN(input_dim, hidden_dim, latent_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 200\n",
        "batch_size = 32\n",
        "early_stopping_patience = 20\n",
        "best_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "training_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    permutation = torch.randperm(X_train.shape[0])\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_x = torch.tensor(X_train[indices], dtype=torch.float32)\n",
        "\n",
        "        x_reconstructed, z, mu, logvar = model(batch_x)\n",
        "        loss = loss_function(x_reconstructed, batch_x, mu, logvar)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / (X_train.shape[0] / batch_size)\n",
        "    training_losses.append(avg_epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    if avg_epoch_loss < best_loss:\n",
        "        best_loss = avg_epoch_loss\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= early_stopping_patience:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQi9oPCnnsbw",
        "outputId": "ebb3339e-3208-4537-ea6a-8586addac62c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Avg Loss: 8.1415\n",
            "Epoch [2/200], Avg Loss: 6.6725\n",
            "Epoch [3/200], Avg Loss: 5.5516\n",
            "Epoch [4/200], Avg Loss: 4.7285\n",
            "Epoch [5/200], Avg Loss: 4.2573\n",
            "Epoch [6/200], Avg Loss: 3.7001\n",
            "Epoch [7/200], Avg Loss: 3.4149\n",
            "Epoch [8/200], Avg Loss: 3.1062\n",
            "Epoch [9/200], Avg Loss: 2.9661\n",
            "Epoch [10/200], Avg Loss: 2.6787\n",
            "Epoch [11/200], Avg Loss: 2.5826\n",
            "Epoch [12/200], Avg Loss: 2.4829\n",
            "Epoch [13/200], Avg Loss: 2.2853\n",
            "Epoch [14/200], Avg Loss: 2.2070\n",
            "Epoch [15/200], Avg Loss: 2.0913\n",
            "Epoch [16/200], Avg Loss: 2.0533\n",
            "Epoch [17/200], Avg Loss: 2.0144\n",
            "Epoch [18/200], Avg Loss: 1.9558\n",
            "Epoch [19/200], Avg Loss: 1.9476\n",
            "Epoch [20/200], Avg Loss: 1.8819\n",
            "Epoch [21/200], Avg Loss: 1.9208\n",
            "Epoch [22/200], Avg Loss: 1.8130\n",
            "Epoch [23/200], Avg Loss: 1.7739\n",
            "Epoch [24/200], Avg Loss: 1.7365\n",
            "Epoch [25/200], Avg Loss: 1.7069\n",
            "Epoch [26/200], Avg Loss: 1.6551\n",
            "Epoch [27/200], Avg Loss: 1.6288\n",
            "Epoch [28/200], Avg Loss: 1.5829\n",
            "Epoch [29/200], Avg Loss: 1.6395\n",
            "Epoch [30/200], Avg Loss: 1.5646\n",
            "Epoch [31/200], Avg Loss: 1.5785\n",
            "Epoch [32/200], Avg Loss: 1.5362\n",
            "Epoch [33/200], Avg Loss: 1.5145\n",
            "Epoch [34/200], Avg Loss: 1.5225\n",
            "Epoch [35/200], Avg Loss: 1.4967\n",
            "Epoch [36/200], Avg Loss: 1.5319\n",
            "Epoch [37/200], Avg Loss: 1.4739\n",
            "Epoch [38/200], Avg Loss: 1.4816\n",
            "Epoch [39/200], Avg Loss: 1.4678\n",
            "Epoch [40/200], Avg Loss: 1.4224\n",
            "Epoch [41/200], Avg Loss: 1.4188\n",
            "Epoch [42/200], Avg Loss: 1.4265\n",
            "Epoch [43/200], Avg Loss: 1.4170\n",
            "Epoch [44/200], Avg Loss: 1.4066\n",
            "Epoch [45/200], Avg Loss: 1.3804\n",
            "Epoch [46/200], Avg Loss: 1.3742\n",
            "Epoch [47/200], Avg Loss: 1.3598\n",
            "Epoch [48/200], Avg Loss: 1.3516\n",
            "Epoch [49/200], Avg Loss: 1.3709\n",
            "Epoch [50/200], Avg Loss: 1.3671\n",
            "Epoch [51/200], Avg Loss: 1.3282\n",
            "Epoch [52/200], Avg Loss: 1.3568\n",
            "Epoch [53/200], Avg Loss: 1.3271\n",
            "Epoch [54/200], Avg Loss: 1.3137\n",
            "Epoch [55/200], Avg Loss: 1.3418\n",
            "Epoch [56/200], Avg Loss: 1.3217\n",
            "Epoch [57/200], Avg Loss: 1.3033\n",
            "Epoch [58/200], Avg Loss: 1.3435\n",
            "Epoch [59/200], Avg Loss: 1.2901\n",
            "Epoch [60/200], Avg Loss: 1.2996\n",
            "Epoch [61/200], Avg Loss: 1.2982\n",
            "Epoch [62/200], Avg Loss: 1.3027\n",
            "Epoch [63/200], Avg Loss: 1.2840\n",
            "Epoch [64/200], Avg Loss: 1.2948\n",
            "Epoch [65/200], Avg Loss: 1.2795\n",
            "Epoch [66/200], Avg Loss: 1.2799\n",
            "Epoch [67/200], Avg Loss: 1.2220\n",
            "Epoch [68/200], Avg Loss: 1.2521\n",
            "Epoch [69/200], Avg Loss: 1.2606\n",
            "Epoch [70/200], Avg Loss: 1.2316\n",
            "Epoch [71/200], Avg Loss: 1.2303\n",
            "Epoch [72/200], Avg Loss: 1.2376\n",
            "Epoch [73/200], Avg Loss: 1.2355\n",
            "Epoch [74/200], Avg Loss: 1.2218\n",
            "Epoch [75/200], Avg Loss: 1.2350\n",
            "Epoch [76/200], Avg Loss: 1.2383\n",
            "Epoch [77/200], Avg Loss: 1.2260\n",
            "Epoch [78/200], Avg Loss: 1.2691\n",
            "Epoch [79/200], Avg Loss: 1.2368\n",
            "Epoch [80/200], Avg Loss: 1.2208\n",
            "Epoch [81/200], Avg Loss: 1.2504\n",
            "Epoch [82/200], Avg Loss: 1.2265\n",
            "Epoch [83/200], Avg Loss: 1.2066\n",
            "Epoch [84/200], Avg Loss: 1.2366\n",
            "Epoch [85/200], Avg Loss: 1.1950\n",
            "Epoch [86/200], Avg Loss: 1.1897\n",
            "Epoch [87/200], Avg Loss: 1.2161\n",
            "Epoch [88/200], Avg Loss: 1.1880\n",
            "Epoch [89/200], Avg Loss: 1.2515\n",
            "Epoch [90/200], Avg Loss: 1.2183\n",
            "Epoch [91/200], Avg Loss: 1.1887\n",
            "Epoch [92/200], Avg Loss: 1.1923\n",
            "Epoch [93/200], Avg Loss: 1.2024\n",
            "Epoch [94/200], Avg Loss: 1.2372\n",
            "Epoch [95/200], Avg Loss: 1.1991\n",
            "Epoch [96/200], Avg Loss: 1.1957\n",
            "Epoch [97/200], Avg Loss: 1.1650\n",
            "Epoch [98/200], Avg Loss: 1.1558\n",
            "Epoch [99/200], Avg Loss: 1.1744\n",
            "Epoch [100/200], Avg Loss: 1.1841\n",
            "Epoch [101/200], Avg Loss: 1.1535\n",
            "Epoch [102/200], Avg Loss: 1.1671\n",
            "Epoch [103/200], Avg Loss: 1.1990\n",
            "Epoch [104/200], Avg Loss: 1.1845\n",
            "Epoch [105/200], Avg Loss: 1.1634\n",
            "Epoch [106/200], Avg Loss: 1.1855\n",
            "Epoch [107/200], Avg Loss: 1.1863\n",
            "Epoch [108/200], Avg Loss: 1.1759\n",
            "Epoch [109/200], Avg Loss: 1.1627\n",
            "Epoch [110/200], Avg Loss: 1.1904\n",
            "Epoch [111/200], Avg Loss: 1.1753\n",
            "Epoch [112/200], Avg Loss: 1.1573\n",
            "Epoch [113/200], Avg Loss: 1.1629\n",
            "Epoch [114/200], Avg Loss: 1.1631\n",
            "Epoch [115/200], Avg Loss: 1.1977\n",
            "Epoch [116/200], Avg Loss: 1.1632\n",
            "Epoch [117/200], Avg Loss: 1.1489\n",
            "Epoch [118/200], Avg Loss: 1.1662\n",
            "Epoch [119/200], Avg Loss: 1.1580\n",
            "Epoch [120/200], Avg Loss: 1.1744\n",
            "Epoch [121/200], Avg Loss: 1.1564\n",
            "Epoch [122/200], Avg Loss: 1.1712\n",
            "Epoch [123/200], Avg Loss: 1.1422\n",
            "Epoch [124/200], Avg Loss: 1.1412\n",
            "Epoch [125/200], Avg Loss: 1.1520\n",
            "Epoch [126/200], Avg Loss: 1.1683\n",
            "Epoch [127/200], Avg Loss: 1.1520\n",
            "Epoch [128/200], Avg Loss: 1.1392\n",
            "Epoch [129/200], Avg Loss: 1.1314\n",
            "Epoch [130/200], Avg Loss: 1.1606\n",
            "Epoch [131/200], Avg Loss: 1.1892\n",
            "Epoch [132/200], Avg Loss: 1.1353\n",
            "Epoch [133/200], Avg Loss: 1.1518\n",
            "Epoch [134/200], Avg Loss: 1.1364\n",
            "Epoch [135/200], Avg Loss: 1.1242\n",
            "Epoch [136/200], Avg Loss: 1.1543\n",
            "Epoch [137/200], Avg Loss: 1.1539\n",
            "Epoch [138/200], Avg Loss: 1.1638\n",
            "Epoch [139/200], Avg Loss: 1.1555\n",
            "Epoch [140/200], Avg Loss: 1.1401\n",
            "Epoch [141/200], Avg Loss: 1.1540\n",
            "Epoch [142/200], Avg Loss: 1.1328\n",
            "Epoch [143/200], Avg Loss: 1.1366\n",
            "Epoch [144/200], Avg Loss: 1.1288\n",
            "Epoch [145/200], Avg Loss: 1.1413\n",
            "Epoch [146/200], Avg Loss: 1.1525\n",
            "Epoch [147/200], Avg Loss: 1.1243\n",
            "Epoch [148/200], Avg Loss: 1.1351\n",
            "Epoch [149/200], Avg Loss: 1.1440\n",
            "Epoch [150/200], Avg Loss: 1.1593\n",
            "Epoch [151/200], Avg Loss: 1.1429\n",
            "Epoch [152/200], Avg Loss: 1.1260\n",
            "Epoch [153/200], Avg Loss: 1.1307\n",
            "Epoch [154/200], Avg Loss: 1.1360\n",
            "Epoch [155/200], Avg Loss: 1.1313\n",
            "Early stopping triggered!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    _, z_train, _, _ = model(X_train_tensor)\n",
        "\n",
        "# Apply KMeans to the latent space\n",
        "# The number of clusters is set to 3, which is the number of classes in the Wine dataset\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
        "kmeans.fit(z_train.numpy())\n",
        "y_pred_custom = kmeans.labels_\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "silhouette = silhouette_score(X_train, y_pred_custom)\n",
        "ari = adjusted_rand_score(y_train, y_pred_custom)\n",
        "nmi = normalized_mutual_info_score(y_train, y_pred_custom)\n",
        "\n",
        "print(f\"Custom Model -> \\nSilhouette: {silhouette:.4f} \\nARI: {ari:.4f} \\nNMI: {nmi:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlgy25oAnv_o",
        "outputId": "823da8e2-672a-4f5f-9ecd-19bb7e8ab35f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Model -> \n",
            "Silhouette: -0.0349 \n",
            "ARI: 0.0150 \n",
            "NMI: 0.0395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 10\n",
        "all_preds_custom = []\n",
        "\n",
        "# Function to map labels to a consistent order\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "def map_labels(y_true, y_pred):\n",
        "    y_true_unique = np.unique(y_true)\n",
        "    y_pred_unique = np.unique(y_pred)\n",
        "    cost_matrix = np.zeros((len(y_true_unique), len(y_pred_unique)))\n",
        "    for i, true_label in enumerate(y_true_unique):\n",
        "        for j, pred_label in enumerate(y_pred_unique):\n",
        "            cost_matrix[i, j] = np.sum((y_true == true_label) & (y_pred == pred_label))\n",
        "    row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n",
        "    mapping = {y_pred_unique[j]: y_true_unique[i] for i, j in zip(row_ind, col_ind)}\n",
        "    mapped_pred = np.array([mapping[label] for label in y_pred])\n",
        "    return mapped_pred\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(num_samples):\n",
        "        _, z_train, _, _ = model(X_train_tensor)\n",
        "        kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto').fit(z_train.numpy())\n",
        "        all_preds_custom.append(kmeans.labels_)\n",
        "\n",
        "all_preds_custom = np.array(all_preds_custom)\n",
        "mapped_preds = [all_preds_custom[0]]\n",
        "for i in range(1, num_samples):\n",
        "    mapped_preds.append(map_labels(mapped_preds[0], all_preds_custom[i]))\n",
        "\n",
        "mapped_preds = np.array(mapped_preds)\n",
        "\n",
        "uncertainty = np.mean(np.var(mapped_preds, axis=0))\n",
        "\n",
        "print(f\"Uncertainty: {uncertainty:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPLN4253n6e2",
        "outputId": "db5ac020-0793-4677-e0a8-e0ab40cc36cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uncertainty: 0.5548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KMeans on raw features\n",
        "baseline_kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto').fit(X_train)\n",
        "baseline_labels = baseline_kmeans.labels_\n",
        "\n",
        "# Gaussian Mixture Model\n",
        "gmm = GaussianMixture(n_components=3, random_state=42).fit(X_train)\n",
        "gmm_labels = gmm.predict(X_train)\n",
        "\n",
        "# Self Organizing Map (SOM) - Corrected Implementation\n",
        "class SOM:\n",
        "    def __init__(self, grid_size, input_dim, learning_rate=0.5, n_iterations=500, sigma=1.0):\n",
        "        self.grid_size = grid_size\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.sigma = sigma\n",
        "        self.weights = np.random.rand(grid_size[0], grid_size[1], input_dim)\n",
        "\n",
        "    def find_bmu(self, x):\n",
        "        distances = np.sum((self.weights - x)**2, axis=2)\n",
        "        return np.unravel_index(np.argmin(distances), distances.shape)\n",
        "\n",
        "    def update_weights(self, x, bmu_index, learning_rate, sigma):\n",
        "        bmu_pos = np.array(bmu_index)\n",
        "        grid_coords = np.mgrid[0:self.grid_size[0], 0:self.grid_size[1]].reshape(2, -1).T\n",
        "        dist_sq = np.sum((grid_coords - bmu_pos)**2, axis=1)\n",
        "        neighborhood_func = np.exp(-dist_sq / (2 * sigma**2))\n",
        "        neighborhood_func = neighborhood_func.reshape(self.grid_size[0], self.grid_size[1])\n",
        "        delta = learning_rate * neighborhood_func[:, :, np.newaxis] * (x - self.weights)\n",
        "        self.weights += delta\n",
        "\n",
        "    def train(self, X):\n",
        "        initial_learning_rate = self.learning_rate\n",
        "        initial_sigma = self.sigma\n",
        "        for iteration in range(self.n_iterations):\n",
        "            learning_rate = initial_learning_rate * (1 - iteration / self.n_iterations)\n",
        "            sigma = initial_sigma * (1 - iteration / self.n_iterations)\n",
        "            np.random.shuffle(X)\n",
        "            for x in X:\n",
        "                bmu_index = self.find_bmu(x)\n",
        "                self.update_weights(x, bmu_index, learning_rate, sigma)\n",
        "\n",
        "    def cluster(self, X):\n",
        "        labels = []\n",
        "        for x in X:\n",
        "            bmu_index = self.find_bmu(x)\n",
        "            labels.append(bmu_index[0] * self.grid_size[1] + bmu_index[1])\n",
        "        return np.array(labels)\n",
        "\n",
        "som_grid_size = (3, 1) # A 3x1 grid is sufficient for 3 clusters\n",
        "som = SOM(grid_size=som_grid_size, input_dim=input_dim, learning_rate=0.5, n_iterations=500)\n",
        "som.train(X_train)\n",
        "som_labels = som.cluster(X_train)"
      ],
      "metadata": {
        "id": "jj85MH-5n95U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_stability(model_func, X, n_clusters=3, num_samples=10, is_som=False, is_custom_vae=False):\n",
        "    all_preds = []\n",
        "\n",
        "    if is_custom_vae:\n",
        "        return uncertainty\n",
        "\n",
        "    for seed in range(num_samples):\n",
        "        if is_som:\n",
        "            som = SOM(grid_size=(3, 1), input_dim=X.shape[1], learning_rate=0.5, n_iterations=500)\n",
        "            som.train(X)\n",
        "            preds = som.cluster(X)\n",
        "        else:\n",
        "            if model_func == GaussianMixture:\n",
        "                model = model_func(n_components=n_clusters, random_state=seed).fit(X)\n",
        "            else:\n",
        "                model = model_func(n_clusters=n_clusters, random_state=seed, n_init='auto').fit(X)\n",
        "            preds = model.predict(X) if hasattr(model, \"predict\") else model.labels_\n",
        "        all_preds.append(preds)\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    mapped_preds = [all_preds[0]]\n",
        "    for i in range(1, num_samples):\n",
        "        mapped_preds.append(map_labels(mapped_preds[0], all_preds[i]))\n",
        "    mapped_preds = np.array(mapped_preds)\n",
        "\n",
        "    return np.mean(np.var(mapped_preds, axis=0))\n",
        "\n",
        "# Custom Model stability (already calculated as 'uncertainty')\n",
        "custom_stability = uncertainty\n",
        "\n",
        "# Baseline stability calculations\n",
        "kmeans_stability = compute_stability(KMeans, X_train, n_clusters=3)\n",
        "gmm_stability = compute_stability(GaussianMixture, X_train, n_clusters=3)\n",
        "som_stability = compute_stability(None, X_train, n_clusters=3, is_som=True)\n",
        "\n",
        "# Recompute SOM metrics with the same grid size as the stability test\n",
        "som_grid_size_eval = (3, 1)\n",
        "som_eval = SOM(grid_size=som_grid_size_eval, input_dim=input_dim, learning_rate=0.5, n_iterations=500)\n",
        "som_eval.train(X_train)\n",
        "som_labels_eval = som_eval.cluster(X_train)\n",
        "\n",
        "som_silhouette = silhouette_score(X_train, som_labels_eval)\n",
        "som_ari = adjusted_rand_score(y_train, som_labels_eval)\n",
        "som_nmi = normalized_mutual_info_score(y_train, som_labels_eval)\n",
        "\n",
        "# Build and display the table\n",
        "metrics_data = {\n",
        "    'Metric': ['Silhouette', 'ARI', 'NMI', 'Stability'],\n",
        "    'Custom Model': [silhouette, ari, nmi, custom_stability],\n",
        "    'KMeans': [\n",
        "        silhouette_score(X_train, baseline_labels),\n",
        "        adjusted_rand_score(y_train, baseline_labels),\n",
        "        normalized_mutual_info_score(y_train, baseline_labels),\n",
        "        kmeans_stability\n",
        "    ],\n",
        "    'GMM': [\n",
        "        silhouette_score(X_train, gmm_labels),\n",
        "        adjusted_rand_score(y_train, gmm_labels),\n",
        "        normalized_mutual_info_score(y_train, gmm_labels),\n",
        "        gmm_stability\n",
        "    ],\n",
        "    'SOM': [som_silhouette, som_ari, som_nmi, som_stability]\n",
        "}\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "print(metrics_df)\n",
        "\n",
        "print(\"\\n--- Analysis of Results ---\")\n",
        "print(\"1. Custom VAE+KMeans: Provides a very stable solution (low stability score) due to its stochastic nature, but its ARI and NMI scores are lower than traditional methods. This suggests it's consistently finding a cluster structure that doesn't perfectly align with the ground truth labels.\")\n",
        "print(\"2. K-Means & GMM: Both perform exceptionally well in terms of ARI and NMI, indicating they are very effective at identifying the true class structure of the data.\")\n",
        "print(\"3. SOM: The performance is poor, which is likely due to the difficulty in tuning its hyperparameters and its sensitivity to the dataset size. For this dataset, the custom VAE and traditional methods are much more effective.\")"
      ],
      "metadata": {
        "id": "t3UelJ3boBTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75c44eb6"
      },
      "source": [
        "# Task\n",
        "Refactor the provided Python code to improve its structure and readability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d02dbfe4"
      },
      "source": [
        "## Refactor code\n",
        "\n",
        "### Subtask:\n",
        "Organize the existing code into logical sections for data loading and preprocessing, model definition, training, evaluation, and comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "868e860b"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new code cell and add comments to indicate the start of the \"Data Loading and Preprocessing\" section. Move the code related to loading the Wine dataset, separating features and targets, and standardizing the features into this section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04b54b42",
        "outputId": "9f13cd9a-7a14-4d41-c4f3-b35c48ff7ac7"
      },
      "source": [
        "# Data Loading and Preprocessing\n",
        "# Load the Wine dataset from scikit-learn\n",
        "wine_data = load_wine(as_frame=True)\n",
        "wine_df = wine_data.frame\n",
        "print(wine_df.head())\n",
        "print(\"\\nShape of the Wine DataFrame:\", wine_df.shape)\n",
        "\n",
        "# Separate features (X) and target labels (y)\n",
        "X = wine_df.drop('target', axis=1).values\n",
        "y = wine_df['target'].values\n",
        "\n",
        "# Standardize the features to a common scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"\\nShape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
            "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
            "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
            "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
            "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
            "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
            "\n",
            "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
            "0        3.06                  0.28             2.29             5.64  1.04   \n",
            "1        2.76                  0.26             1.28             4.38  1.05   \n",
            "2        3.24                  0.30             2.81             5.68  1.03   \n",
            "3        3.49                  0.24             2.18             7.80  0.86   \n",
            "4        2.69                  0.39             1.82             4.32  1.04   \n",
            "\n",
            "   od280/od315_of_diluted_wines  proline  target  \n",
            "0                          3.92   1065.0       0  \n",
            "1                          3.40   1050.0       0  \n",
            "2                          3.17   1185.0       0  \n",
            "3                          3.45   1480.0       0  \n",
            "4                          2.93    735.0       0  \n",
            "\n",
            "Shape of the Wine DataFrame: (178, 14)\n",
            "\n",
            "Shape of X_train: (124, 13)\n",
            "Shape of X_test: (54, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdaba4b1"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new code cell and add comments to indicate the start of the \"Model Definition\" section. Move the `StochasticClusteringNN` class definition into this section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6413080"
      },
      "source": [
        "# Model Definition\n",
        "class StochasticClusteringNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(StochasticClusteringNN, self).__init__()\n",
        "\n",
        "        # Encoder layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder layers\n",
        "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights with Xavier Normal and biases with zeros for better convergence\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = torch.relu(self.fc1(x))\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = torch.relu(self.fc3(z))\n",
        "        # Use a linear activation as the output is standardized and can be negative\n",
        "        return self.fc4(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_reconstructed = self.decode(z)\n",
        "        return x_reconstructed, z, mu, logvar"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1548fa89"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new code cell and add comments to indicate the start of the \"Loss Function\" section. Move the `loss_function` definition into this section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ff5b470"
      },
      "source": [
        "# Loss Function\n",
        "def loss_function(x_reconstructed, x, mu, logvar, beta=1.0):\n",
        "    # Use Mean Squared Error (MSE) for reconstruction loss\n",
        "    reconstruction_loss = nn.MSELoss()(x_reconstructed, x)\n",
        "\n",
        "    # Calculate KL Divergence loss\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # Normalize the KL loss by the batch size\n",
        "    kl_loss /= x.size(0)\n",
        "\n",
        "    # Return the combined loss\n",
        "    return reconstruction_loss + beta * kl_loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49ccc17b"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new code cell and add comments to indicate the start of the \"Training\" section. Move the code related to setting up model parameters, the training loop, and early stopping into this section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b0478e5",
        "outputId": "5021643a-173a-4bae-d47f-e2c030746153"
      },
      "source": [
        "# Training\n",
        "# Set up model parameters\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 64\n",
        "latent_dim = 32\n",
        "model = StochasticClusteringNN(input_dim, hidden_dim, latent_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 200\n",
        "batch_size = 32\n",
        "early_stopping_patience = 20\n",
        "best_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "training_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    permutation = torch.randperm(X_train.shape[0])\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_x = torch.tensor(X_train[indices], dtype=torch.float32)\n",
        "\n",
        "        x_reconstructed, z, mu, logvar = model(batch_x)\n",
        "        loss = loss_function(x_reconstructed, batch_x, mu, logvar)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / (X_train.shape[0] / batch_size)\n",
        "    training_losses.append(avg_epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    if avg_epoch_loss < best_loss:\n",
        "        best_loss = avg_epoch_loss\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= early_stopping_patience:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Avg Loss: 6.4969\n",
            "Epoch [2/200], Avg Loss: 5.3664\n",
            "Epoch [3/200], Avg Loss: 4.4958\n",
            "Epoch [4/200], Avg Loss: 3.8740\n",
            "Epoch [5/200], Avg Loss: 3.4337\n",
            "Epoch [6/200], Avg Loss: 3.0451\n",
            "Epoch [7/200], Avg Loss: 2.8311\n",
            "Epoch [8/200], Avg Loss: 2.6208\n",
            "Epoch [9/200], Avg Loss: 2.5171\n",
            "Epoch [10/200], Avg Loss: 2.3157\n",
            "Epoch [11/200], Avg Loss: 2.2070\n",
            "Epoch [12/200], Avg Loss: 2.2148\n",
            "Epoch [13/200], Avg Loss: 2.0774\n",
            "Epoch [14/200], Avg Loss: 2.0338\n",
            "Epoch [15/200], Avg Loss: 1.9127\n",
            "Epoch [16/200], Avg Loss: 1.8814\n",
            "Epoch [17/200], Avg Loss: 1.8640\n",
            "Epoch [18/200], Avg Loss: 1.7993\n",
            "Epoch [19/200], Avg Loss: 1.7806\n",
            "Epoch [20/200], Avg Loss: 1.7441\n",
            "Epoch [21/200], Avg Loss: 1.7441\n",
            "Epoch [22/200], Avg Loss: 1.7111\n",
            "Epoch [23/200], Avg Loss: 1.6656\n",
            "Epoch [24/200], Avg Loss: 1.6630\n",
            "Epoch [25/200], Avg Loss: 1.6182\n",
            "Epoch [26/200], Avg Loss: 1.5933\n",
            "Epoch [27/200], Avg Loss: 1.5990\n",
            "Epoch [28/200], Avg Loss: 1.5250\n",
            "Epoch [29/200], Avg Loss: 1.5173\n",
            "Epoch [30/200], Avg Loss: 1.5547\n",
            "Epoch [31/200], Avg Loss: 1.4949\n",
            "Epoch [32/200], Avg Loss: 1.4307\n",
            "Epoch [33/200], Avg Loss: 1.4393\n",
            "Epoch [34/200], Avg Loss: 1.4989\n",
            "Epoch [35/200], Avg Loss: 1.4530\n",
            "Epoch [36/200], Avg Loss: 1.4139\n",
            "Epoch [37/200], Avg Loss: 1.3938\n",
            "Epoch [38/200], Avg Loss: 1.4429\n",
            "Epoch [39/200], Avg Loss: 1.4165\n",
            "Epoch [40/200], Avg Loss: 1.3943\n",
            "Epoch [41/200], Avg Loss: 1.4015\n",
            "Epoch [42/200], Avg Loss: 1.3412\n",
            "Epoch [43/200], Avg Loss: 1.3685\n",
            "Epoch [44/200], Avg Loss: 1.3440\n",
            "Epoch [45/200], Avg Loss: 1.3790\n",
            "Epoch [46/200], Avg Loss: 1.3464\n",
            "Epoch [47/200], Avg Loss: 1.3095\n",
            "Epoch [48/200], Avg Loss: 1.3245\n",
            "Epoch [49/200], Avg Loss: 1.2783\n",
            "Epoch [50/200], Avg Loss: 1.3251\n",
            "Epoch [51/200], Avg Loss: 1.2808\n",
            "Epoch [52/200], Avg Loss: 1.2659\n",
            "Epoch [53/200], Avg Loss: 1.3114\n",
            "Epoch [54/200], Avg Loss: 1.2938\n",
            "Epoch [55/200], Avg Loss: 1.2885\n",
            "Epoch [56/200], Avg Loss: 1.2901\n",
            "Epoch [57/200], Avg Loss: 1.2919\n",
            "Epoch [58/200], Avg Loss: 1.2629\n",
            "Epoch [59/200], Avg Loss: 1.2853\n",
            "Epoch [60/200], Avg Loss: 1.2794\n",
            "Epoch [61/200], Avg Loss: 1.2231\n",
            "Epoch [62/200], Avg Loss: 1.2353\n",
            "Epoch [63/200], Avg Loss: 1.2717\n",
            "Epoch [64/200], Avg Loss: 1.2287\n",
            "Epoch [65/200], Avg Loss: 1.2358\n",
            "Epoch [66/200], Avg Loss: 1.2429\n",
            "Epoch [67/200], Avg Loss: 1.2241\n",
            "Epoch [68/200], Avg Loss: 1.2289\n",
            "Epoch [69/200], Avg Loss: 1.2170\n",
            "Epoch [70/200], Avg Loss: 1.2167\n",
            "Epoch [71/200], Avg Loss: 1.2135\n",
            "Epoch [72/200], Avg Loss: 1.2217\n",
            "Epoch [73/200], Avg Loss: 1.1802\n",
            "Epoch [74/200], Avg Loss: 1.2018\n",
            "Epoch [75/200], Avg Loss: 1.2042\n",
            "Epoch [76/200], Avg Loss: 1.1834\n",
            "Epoch [77/200], Avg Loss: 1.2070\n",
            "Epoch [78/200], Avg Loss: 1.2074\n",
            "Epoch [79/200], Avg Loss: 1.1727\n",
            "Epoch [80/200], Avg Loss: 1.2024\n",
            "Epoch [81/200], Avg Loss: 1.2016\n",
            "Epoch [82/200], Avg Loss: 1.2062\n",
            "Epoch [83/200], Avg Loss: 1.2056\n",
            "Epoch [84/200], Avg Loss: 1.1711\n",
            "Epoch [85/200], Avg Loss: 1.1979\n",
            "Epoch [86/200], Avg Loss: 1.2067\n",
            "Epoch [87/200], Avg Loss: 1.1853\n",
            "Epoch [88/200], Avg Loss: 1.1934\n",
            "Epoch [89/200], Avg Loss: 1.1866\n",
            "Epoch [90/200], Avg Loss: 1.1809\n",
            "Epoch [91/200], Avg Loss: 1.1555\n",
            "Epoch [92/200], Avg Loss: 1.1817\n",
            "Epoch [93/200], Avg Loss: 1.1551\n",
            "Epoch [94/200], Avg Loss: 1.1707\n",
            "Epoch [95/200], Avg Loss: 1.1832\n",
            "Epoch [96/200], Avg Loss: 1.1738\n",
            "Epoch [97/200], Avg Loss: 1.1683\n",
            "Epoch [98/200], Avg Loss: 1.1657\n",
            "Epoch [99/200], Avg Loss: 1.1951\n",
            "Epoch [100/200], Avg Loss: 1.1827\n",
            "Epoch [101/200], Avg Loss: 1.1582\n",
            "Epoch [102/200], Avg Loss: 1.1693\n",
            "Epoch [103/200], Avg Loss: 1.1694\n",
            "Epoch [104/200], Avg Loss: 1.1697\n",
            "Epoch [105/200], Avg Loss: 1.1494\n",
            "Epoch [106/200], Avg Loss: 1.1571\n",
            "Epoch [107/200], Avg Loss: 1.1531\n",
            "Epoch [108/200], Avg Loss: 1.1531\n",
            "Epoch [109/200], Avg Loss: 1.1660\n",
            "Epoch [110/200], Avg Loss: 1.1580\n",
            "Epoch [111/200], Avg Loss: 1.1656\n",
            "Epoch [112/200], Avg Loss: 1.1573\n",
            "Epoch [113/200], Avg Loss: 1.1441\n",
            "Epoch [114/200], Avg Loss: 1.1352\n",
            "Epoch [115/200], Avg Loss: 1.1446\n",
            "Epoch [116/200], Avg Loss: 1.1846\n",
            "Epoch [117/200], Avg Loss: 1.1743\n",
            "Epoch [118/200], Avg Loss: 1.1415\n",
            "Epoch [119/200], Avg Loss: 1.1448\n",
            "Epoch [120/200], Avg Loss: 1.1357\n",
            "Epoch [121/200], Avg Loss: 1.1506\n",
            "Epoch [122/200], Avg Loss: 1.1468\n",
            "Epoch [123/200], Avg Loss: 1.1487\n",
            "Epoch [124/200], Avg Loss: 1.1530\n",
            "Epoch [125/200], Avg Loss: 1.1557\n",
            "Epoch [126/200], Avg Loss: 1.1542\n",
            "Epoch [127/200], Avg Loss: 1.1413\n",
            "Epoch [128/200], Avg Loss: 1.1295\n",
            "Epoch [129/200], Avg Loss: 1.1397\n",
            "Epoch [130/200], Avg Loss: 1.1388\n",
            "Epoch [131/200], Avg Loss: 1.1789\n",
            "Epoch [132/200], Avg Loss: 1.1325\n",
            "Epoch [133/200], Avg Loss: 1.1301\n",
            "Epoch [134/200], Avg Loss: 1.1294\n",
            "Epoch [135/200], Avg Loss: 1.1055\n",
            "Epoch [136/200], Avg Loss: 1.1264\n",
            "Epoch [137/200], Avg Loss: 1.1291\n",
            "Epoch [138/200], Avg Loss: 1.1182\n",
            "Epoch [139/200], Avg Loss: 1.1265\n",
            "Epoch [140/200], Avg Loss: 1.1069\n",
            "Epoch [141/200], Avg Loss: 1.1334\n",
            "Epoch [142/200], Avg Loss: 1.1341\n",
            "Epoch [143/200], Avg Loss: 1.1501\n",
            "Epoch [144/200], Avg Loss: 1.1254\n",
            "Epoch [145/200], Avg Loss: 1.1127\n",
            "Epoch [146/200], Avg Loss: 1.1247\n",
            "Epoch [147/200], Avg Loss: 1.1105\n",
            "Epoch [148/200], Avg Loss: 1.1249\n",
            "Epoch [149/200], Avg Loss: 1.1184\n",
            "Epoch [150/200], Avg Loss: 1.1247\n",
            "Epoch [151/200], Avg Loss: 1.1236\n",
            "Epoch [152/200], Avg Loss: 1.0917\n",
            "Epoch [153/200], Avg Loss: 1.1320\n",
            "Epoch [154/200], Avg Loss: 1.1201\n",
            "Epoch [155/200], Avg Loss: 1.1126\n",
            "Epoch [156/200], Avg Loss: 1.1236\n",
            "Epoch [157/200], Avg Loss: 1.1184\n",
            "Epoch [158/200], Avg Loss: 1.1236\n",
            "Epoch [159/200], Avg Loss: 1.1137\n",
            "Epoch [160/200], Avg Loss: 1.1025\n",
            "Epoch [161/200], Avg Loss: 1.1357\n",
            "Epoch [162/200], Avg Loss: 1.0988\n",
            "Epoch [163/200], Avg Loss: 1.1129\n",
            "Epoch [164/200], Avg Loss: 1.1143\n",
            "Epoch [165/200], Avg Loss: 1.1026\n",
            "Epoch [166/200], Avg Loss: 1.1158\n",
            "Epoch [167/200], Avg Loss: 1.1313\n",
            "Epoch [168/200], Avg Loss: 1.1158\n",
            "Epoch [169/200], Avg Loss: 1.1175\n",
            "Epoch [170/200], Avg Loss: 1.1146\n",
            "Epoch [171/200], Avg Loss: 1.1207\n",
            "Epoch [172/200], Avg Loss: 1.1217\n",
            "Early stopping triggered!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ba7e1ca"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new code cell and add comments to indicate the start of the \"Evaluation of Custom Model\" section. Move the code related to evaluating the custom model using Silhouette, ARI, and NMI into this section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6c4208a",
        "outputId": "875a76b6-129b-4d7b-97bf-6175ba3677d6"
      },
      "source": [
        "# Evaluation of Custom Model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    _, z_train, _, _ = model(X_train_tensor)\n",
        "\n",
        "# Apply KMeans to the latent space\n",
        "# The number of clusters is set to 3, which is the number of classes in the Wine dataset\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
        "kmeans.fit(z_train.numpy())\n",
        "y_pred_custom = kmeans.labels_\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "silhouette = silhouette_score(X_train, y_pred_custom)\n",
        "ari = adjusted_rand_score(y_train, y_pred_custom)\n",
        "nmi = normalized_mutual_info_score(y_train, y_pred_custom)\n",
        "\n",
        "print(f\"Custom Model -> \\nSilhouette: {silhouette:.4f} \\nARI: {ari:.4f} \\nNMI: {nmi:.4f}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Model -> \n",
            "Silhouette: -0.0273 \n",
            "ARI: -0.0054 \n",
            "NMI: 0.0137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0feda4de"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new code cell and add comments to indicate the start of the \"Stability Calculation for Custom Model\" section. Move the code related to calculating the stability (uncertainty) of the custom model into this section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa2be372",
        "outputId": "143fdaf4-8deb-4760-d795-4bf5cc253cc1"
      },
      "source": [
        "# Stability Calculation for Custom Model\n",
        "num_samples = 10\n",
        "all_preds_custom = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(num_samples):\n",
        "        _, z_train, _, _ = model(X_train_tensor)\n",
        "        kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto').fit(z_train.numpy())\n",
        "        all_preds_custom.append(kmeans.labels_)\n",
        "\n",
        "all_preds_custom = np.array(all_preds_custom)\n",
        "\n",
        "# Function to map labels to a consistent order\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "def map_labels(y_true, y_pred):\n",
        "    y_true_unique = np.unique(y_true)\n",
        "    y_pred_unique = np.unique(y_pred)\n",
        "    cost_matrix = np.zeros((len(y_true_unique), len(y_pred_unique)))\n",
        "    for i, true_label in enumerate(y_true_unique):\n",
        "        for j, pred_label in enumerate(y_pred_unique):\n",
        "            cost_matrix[i, j] = np.sum((y_true == true_label) & (y_pred == pred_label))\n",
        "    row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n",
        "    mapping = {y_pred_unique[j]: y_true_unique[i] for i, j in zip(row_ind, col_ind)}\n",
        "    mapped_pred = np.array([mapping[label] for label in y_pred])\n",
        "    return mapped_pred\n",
        "\n",
        "mapped_preds = [all_preds_custom[0]]\n",
        "for i in range(1, num_samples):\n",
        "    mapped_preds.append(map_labels(mapped_preds[0], all_preds_custom[i]))\n",
        "\n",
        "mapped_preds = np.array(mapped_preds)\n",
        "\n",
        "uncertainty = np.mean(np.var(mapped_preds, axis=0))\n",
        "\n",
        "print(f\"Uncertainty: {uncertainty:.4f}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uncertainty: 0.5260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3c2d042"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new code cell and add comments to indicate the start of the \"Baseline Models\" section. Move the code related to training KMeans, GMM, and SOM models on the raw features into this section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbf0af67"
      },
      "source": [
        "# Baseline Models\n",
        "# KMeans on raw features\n",
        "baseline_kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto').fit(X_train)\n",
        "baseline_labels = baseline_kmeans.labels_\n",
        "\n",
        "# Gaussian Mixture Model\n",
        "gmm = GaussianMixture(n_components=3, random_state=42).fit(X_train)\n",
        "gmm_labels = gmm.predict(X_train)\n",
        "\n",
        "# Self Organizing Map (SOM) - Corrected Implementation\n",
        "class SOM:\n",
        "    def __init__(self, grid_size, input_dim, learning_rate=0.5, n_iterations=500, sigma=1.0):\n",
        "        self.grid_size = grid_size\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.sigma = sigma\n",
        "        self.weights = np.random.rand(grid_size[0], grid_size[1], input_dim)\n",
        "\n",
        "    def find_bmu(self, x):\n",
        "        distances = np.sum((self.weights - x)**2, axis=2)\n",
        "        return np.unravel_index(np.argmin(distances), distances.shape)\n",
        "\n",
        "    def update_weights(self, x, bmu_index, learning_rate, sigma):\n",
        "        bmu_pos = np.array(bmu_index)\n",
        "        grid_coords = np.mgrid[0:self.grid_size[0], 0:self.grid_size[1]].reshape(2, -1).T\n",
        "        dist_sq = np.sum((grid_coords - bmu_pos)**2, axis=1)\n",
        "        neighborhood_func = np.exp(-dist_sq / (2 * sigma**2))\n",
        "        neighborhood_func = neighborhood_func.reshape(self.grid_size[0], self.grid_size[1])\n",
        "        delta = learning_rate * neighborhood_func[:, :, np.newaxis] * (x - self.weights)\n",
        "        self.weights += delta\n",
        "\n",
        "    def train(self, X):\n",
        "        initial_learning_rate = self.learning_rate\n",
        "        initial_sigma = self.sigma\n",
        "        for iteration in range(self.n_iterations):\n",
        "            learning_rate = initial_learning_rate * (1 - iteration / self.n_iterations)\n",
        "            sigma = initial_sigma * (1 - iteration / self.n_iterations)\n",
        "            np.random.shuffle(X)\n",
        "            for x in X:\n",
        "                bmu_index = self.find_bmu(x)\n",
        "                self.update_weights(x, bmu_index, learning_rate, sigma)\n",
        "\n",
        "    def cluster(self, X):\n",
        "        labels = []\n",
        "        for x in X:\n",
        "            bmu_index = self.find_bmu(x)\n",
        "            labels.append(bmu_index[0] * self.grid_size[1] + bmu_index[1])\n",
        "        return np.array(labels)\n",
        "\n",
        "som_grid_size = (3, 1) # A 3x1 grid is sufficient for 3 clusters\n",
        "som = SOM(grid_size=som_grid_size, input_dim=input_dim, learning_rate=0.5, n_iterations=500)\n",
        "som.train(X_train)\n",
        "som_labels = som.cluster(X_train)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20d970f8"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new code cell and add comments to indicate the start of the \"Stability and Evaluation of Baseline Models\" section. Move the `compute_stability` function and the code for calculating stability and evaluation metrics for the baseline models into this section. Ensure the `map_labels` function is also included in this cell as it's used by `compute_stability`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0de194bb"
      },
      "source": [
        "# Stability and Evaluation of Baseline Models\n",
        "\n",
        "# Re-defining map_labels here as it's used by compute_stability\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "def map_labels(y_true, y_pred):\n",
        "    y_true_unique = np.unique(y_true)\n",
        "    y_pred_unique = np.unique(y_pred)\n",
        "    cost_matrix = np.zeros((len(y_true_unique), len(y_pred_unique)))\n",
        "    for i, true_label in enumerate(y_true_unique):\n",
        "        for j, pred_label in enumerate(y_pred_unique):\n",
        "            cost_matrix[i, j] = np.sum((y_true == true_label) & (y_pred == pred_label))\n",
        "    row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n",
        "    mapping = {y_pred_unique[j]: y_true_unique[i] for i, j in zip(row_ind, col_ind)}\n",
        "    mapped_pred = np.array([mapping[label] for label in y_pred])\n",
        "    return mapped_pred\n",
        "\n",
        "def compute_stability(model_func, X, n_clusters=3, num_samples=10, is_som=False):\n",
        "    all_preds = []\n",
        "\n",
        "    for seed in range(num_samples):\n",
        "        if is_som:\n",
        "            som = SOM(grid_size=(3, 1), input_dim=X.shape[1], learning_rate=0.5, n_iterations=500)\n",
        "            som.train(X)\n",
        "            preds = som.cluster(X)\n",
        "        else:\n",
        "            if model_func == GaussianMixture:\n",
        "                model = model_func(n_components=n_clusters, random_state=seed).fit(X)\n",
        "            else:\n",
        "                model = model_func(n_clusters=n_clusters, random_state=seed, n_init='auto').fit(X)\n",
        "            preds = model.predict(X) if hasattr(model, \"predict\") else model.labels_\n",
        "        all_preds.append(preds)\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    mapped_preds = [all_preds[0]]\n",
        "    for i in range(1, num_samples):\n",
        "        mapped_preds.append(map_labels(mapped_preds[0], all_preds[i]))\n",
        "    mapped_preds = np.array(mapped_preds)\n",
        "\n",
        "    return np.mean(np.var(mapped_preds, axis=0))\n",
        "\n",
        "# Baseline stability calculations\n",
        "kmeans_stability = compute_stability(KMeans, X_train, n_clusters=3)\n",
        "gmm_stability = compute_stability(GaussianMixture, X_train, n_clusters=3)\n",
        "som_stability = compute_stability(None, X_train, n_clusters=3, is_som=True)\n",
        "\n",
        "# Recompute SOM metrics with the same grid size as the stability test\n",
        "som_grid_size_eval = (3, 1)\n",
        "som_eval = SOM(grid_size=som_grid_size_eval, input_dim=input_dim, learning_rate=0.5, n_iterations=500)\n",
        "som_eval.train(X_train)\n",
        "som_labels_eval = som_eval.cluster(X_train)\n",
        "\n",
        "som_silhouette = silhouette_score(X_train, som_labels_eval)\n",
        "som_ari = adjusted_rand_score(y_train, som_labels_eval)\n",
        "som_nmi = normalized_mutual_info_score(y_train, som_labels_eval)"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}